# -*- coding: utf-8 -*-
"""MNIST MLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14DJZZPXQI9GtnszHq2j6rcRBqOSXvZzm

The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.
"""

import os
#Install TensorFlow
import tensorflow as tf

#Import matplotlib for plotting
import matplotlib.pyplot as plt

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   raise SystemError('GPU device not found')
# print('Found GPU at: {}'.format(device_name))

"""Load the MNIST dataset"""

#Load MNIST Dataset

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

"""Review some dataset values"""

#Print some images and its labels
num = 10
num_row = 2
num_col = 5

images = x_train[:num]
labels = y_train[:num]
# plot images
fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))
for i in range(num):
    ax = axes[i//num_col, i%num_col]
    ax.imshow(images[i], cmap='gray')
    ax.set_title('Label: {}'.format(labels[i]))
plt.tight_layout()
plt.show()

"""Shape of the images:"""

x_train.shape

"""Prepare the data"""

#Normalize values between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

"""Build a Sequential NN with Fully Connected/Dense layers"""

#Simple FF connected 
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

"""For each example the model returns a vector of "logits" or "log-odds" scores, one for each class."""

predictions = model(x_train[:1]).numpy()
predictions

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

loss_fn(y_train[:1], predictions).numpy()

y_train[:1]

model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])

directory = './generated/'
try:
    os.makedirs(directory)
except:
    print("Directory already exists: [" + directory + "]")
tf.keras.utils.plot_model(model, to_file=f"{directory}model_mlp.png", show_shapes=True)

model.summary()

"""Train the model"""

history = model.fit(x_train, y_train, epochs=10, validation_split=0.1)

"""The fit method returns a history object that we can use to understand how the training performed."""

def display_training_curves(training, validation, title, subplot):
  ax = plt.subplot(subplot)
  ax.plot(training)
  ax.plot(validation)
  ax.set_title('model '+ title)
  ax.set_ylabel(title)
  ax.set_xlabel('epoch')
  ax.legend(['training', 'validation'])

plt.subplots(figsize=(10,10))
plt.tight_layout()
display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)
display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)
plt.show()

model.evaluate(x_test,  y_test, verbose=2)

probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

#Some imports to pretty print things
import numpy as np
import pandas as pd
import seaborn as sn

#Probabilitys
np.set_printoptions(formatter={'float': '{: 0.4f}'.format})
probability_model(x_test[:5])

data = {'y_Actual':    y_test,
        'y_Predicted': np.argmax(probability_model(x_test), axis=1)} 
df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])

confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])

sn.heatmap(confusion_matrix, annot=True, fmt="d")
plt.show()